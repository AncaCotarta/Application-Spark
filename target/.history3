1733165790750:run
1733165835415:sbt run -- -rt csv -rc src/main/resources/DataforTest/data.csv
1733165840073:sbt
1733165842426:run
1733165885279:run -- -rt csv -rc src/main/resources/DataforTest/data.csv
1733166002195:sbt:projet_sda_2024> run -- -rt csv -rc src/main/resources/DataforTest/data.csv\n[info] running sda.main.MainBatch -- -rt csv -rc src/main/resources/DataforTest/data.csv\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n24/12/02 19:58:07 WARN Utils: Your hostname, MacBook-Air-de-Nassim.local resolves to a loopback address: 127.0.0.1; using 192.0.0.2 instead (on interface en0)\n24/12/02 19:58:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n24/12/02 19:58:07 INFO SparkContext: Running Spark version 3.3.1\n24/12/02 19:58:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n24/12/02 19:58:07 INFO ResourceUtils: ==============================================================\n24/12/02 19:58:07 INFO ResourceUtils: No custom resources configured for spark.driver.\n24/12/02 19:58:07 INFO ResourceUtils: ==============================================================\n24/12/02 19:58:07 INFO SparkContext: Submitted application: SDA\n24/12/02 19:58:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n24/12/02 19:58:07 INFO ResourceProfile: Limiting resource is cpu\n24/12/02 19:58:07 INFO ResourceProfileManager: Added ResourceProfile id: 0\n24/12/02 19:58:07 INFO SecurityManager: Changing view acls to: nassimsahib\n24/12/02 19:58:07 INFO SecurityManager: Changing modify acls to: nassimsahib\n24/12/02 19:58:07 INFO SecurityManager: Changing view acls groups to: \n24/12/02 19:58:07 INFO SecurityManager: Changing modify acls groups to: \n24/12/02 19:58:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(nassimsahib); groups with view permissions: Set(); users  with modify permissions: Set(nassimsahib); groups with modify permissions: Set()\n24/12/02 19:58:07 INFO Utils: Successfully started service 'sparkDriver' on port 56165.\n24/12/02 19:58:07 INFO SparkEnv: Registering MapOutputTracker\n24/12/02 19:58:07 INFO SparkEnv: Registering BlockManagerMaster\n24/12/02 19:58:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n24/12/02 19:58:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n24/12/02 19:58:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n24/12/02 19:58:07 INFO DiskBlockManager: Created local directory at /private/var/folders/44/jh9fhm3j7k18b_z70c76pcc00000gn/T/blockmgr-7f8751ee-9fde-4bc6-bf52-527ce6932c2f\n24/12/02 19:58:07 INFO MemoryStore: MemoryStore started with capacity 397.5 MiB\n24/12/02 19:58:07 INFO SparkEnv: Registering OutputCommitCoordinator\n24/12/02 19:58:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n24/12/02 19:58:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n24/12/02 19:58:07 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n24/12/02 19:58:07 INFO Executor: Starting executor ID driver on host 192.0.0.2\n24/12/02 19:58:07 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n24/12/02 19:58:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56166.\n24/12/02 19:58:07 INFO NettyBlockTransferService: Server created on 192.0.0.2:56166\n24/12/02 19:58:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n24/12/02 19:58:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.0.0.2, 56166, None)\n24/12/02 19:58:07 INFO BlockManagerMasterEndpoint: Registering block manager 192.0.0.2:56166 with 397.5 MiB RAM, BlockManagerId(driver, 192.0.0.2, 56166, None)\n24/12/02 19:58:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.0.0.2, 56166, None)\n24/12/02 19:58:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.0.0.2, 56166, None)\n[error] com.beust.jcommander.ParameterException: Was passed main parameter '--' but no main parameter was defined\n[error]         at com.beust.jcommander.JCommander.getMainParameter(JCommander.java:914)\n[error]         at com.beust.jcommander.JCommander.parseValues(JCommander.java:759)\n[error]         at com.beust.jcommander.JCommander.parse(JCommander.java:282)\n[error]         at com.beust.jcommander.JCommander.parse(JCommander.java:265)\n[error]         at com.beust.jcommander.JCommander.<init>(JCommander.java:210)\n[error]         at sda.args.Args$.parseArguments(Args.scala:22)\n[error]         at sda.main.MainBatch$.main(MainBatch.scala:16)\n[error]         at sda.main.MainBatch.main(MainBatch.scala)\n[error]         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[error]         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n[error]         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n[error]         at java.lang.reflect.Method.invoke(Method.java:498)\n[error] stack trace is suppressed; run last Compile / run for the full output\n[error] (Compile / run) com.beust.jcommander.ParameterException: Was passed main parameter '--' but no main parameter was defined\n[error] Total time: 2 s, completed 2 dÃ©c. 2024 19:58:07\nsbt:projet_sda_2024> sbt:projet_sda_2024> run -- -rt csv -rc src/main/resources/DataforTest/data.csv\n[info] running sda.main.MainBatch -- -rt csv -rc src/main/resources/DataforTest/data.csv\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n24/12/02 19:58:07 WARN Utils: Your hostname, MacBook-Air-de-Nassim.local resolves to a loopback address: 127.0.0.1; using 192.0.0.2 instead (on interface en0)\n24/12/02 19:58:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n24/12/02 19:58:07 INFO SparkContext: Running Spark version 3.3.1\n24/12/02 19:58:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n24/12/02 19:58:07 INFO ResourceUtils: ==============================================================\n24/12/02 19:58:07 INFO ResourceUtils: No custom resources configured for spark.driver.\n24/12/02 19:58:07 INFO ResourceUtils: ==============================================================\n24/12/02 19:58:07 INFO SparkContext: Submitted application: SDA\n24/12/02 19:58:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n24/12/02 19:58:07 INFO ResourceProfile: Limiting resource is cpu\n24/12/02 19:58:07 INFO ResourceProfileManager: Added ResourceProfile id: 0\n24/12/02 19:58:07 INFO SecurityManager: Changing view acls to: nassimsahib\n24/12/02 19:58:07 INFO SecurityManager: Changing modify acls to: nassimsahib\n24/12/02 19:58:07 INFO SecurityManager: Changing view acls groups to: \n24/12/02 19:58:07 INFO SecurityManager: Changing modify acls groups to: \n24/12/02 19:58:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(nassimsahib); groups with view permissions: Set(); users  with modify permissions: Set(nassimsahib); groups with modify permissions: Set()\n24/12/02 19:58:07 INFO Utils: Successfully started service 'sparkDriver' on port 56165.\n24/12/02 19:58:07 INFO SparkEnv: Registering MapOutputTracker\n24/12/02 19:58:07 INFO SparkEnv: Registering BlockManagerMaster\n24/12/02 19:58:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n24/12/02 19:58:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n24/12/02 19:58:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n24/12/02 19:58:07 INFO DiskBlockManager: Created local directory at /private/var/folders/44/jh9fhm3j7k18b_z70c76pcc00000gn/T/blockmgr-7f8751ee-9fde-4bc6-bf52-527ce6932c2f\n24/12/02 19:58:07 INFO MemoryStore: MemoryStore started with capacity 397.5 MiB\n24/12/02 19:58:07 INFO SparkEnv: Registering OutputCommitCoordinator\n24/12/02 19:58:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n24/12/02 19:58:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n24/12/02 19:58:07 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n24/12/02 19:58:07 INFO Executor: Starting executor ID driver on host 192.0.0.2\n24/12/02 19:58:07 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n24/12/02 19:58:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56166.\n24/12/02 19:58:07 INFO NettyBlockTransferService: Server created on 192.0.0.2:56166\n24/12/02 19:58:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n24/12/02 19:58:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.0.0.2, 56166, None)\n24/12/02 19:58:07 INFO BlockManagerMasterEndpoint: Registering block manager 192.0.0.2:56166 with 397.5 MiB RAM, BlockManagerId(driver, 192.0.0.2, 56166, None)\n24/12/02 19:58:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.0.0.2, 56166, None)\n24/12/02 19:58:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.0.0.2, 56166, None)\n[error] com.beust.jcommander.ParameterException: Was passed main parameter '--' but no main parameter was defined\n[error]         at com.beust.jcommander.JCommander.getMainParameter(JCommander.java:914)\n[error]         at com.beust.jcommander.JCommander.parseValues(JCommander.java:759)\n[error]         at com.beust.jcommander.JCommander.parse(JCommander.java:282)\n[error]         at com.beust.jcommander.JCommander.parse(JCommander.java:265)\n[error]         at com.beust.jcommander.JCommander.<init>(JCommander.java:210)\n[error]         at sda.args.Args$.parseArguments(Args.scala:22)\n[error]         at sda.main.MainBatch$.main(MainBatch.scala:16)\n[error]         at sda.main.MainBatch.main(MainBatch.scala)\n[error]         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[error]         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n[error]         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n[error]         at java.lang.reflect.Method.invoke(Method.java:498)\n[error] stack trace is suppressed; run last Compile / run for the full output\n[error] (Compile / run) com.beust.jcommander.ParameterException: Was passed main parameter '--' but no main parameter was defined\n[error] Total time: 2 s, completed 2 dÃ©c. 2024 19:58:07\nsbt:projet_sda_2024> -rt csv -rc src/main/resources/DataforTest/data.csv
1733166008436:-rt csv -rc src/main/resources/DataforTest/data.csv
1733166023416:sbt run -rt csv -rc src/main/resources/DataforTest/data.csv
1733166026393:sbt
1733166028573:run
1733166074001:-debug run -rt csv -rc src/main/resources/DataforTest/data.csv
1733166091060:sbt -debug run -rt csv -rc src/main/resources/DataforTest/data.csv
1733166205582:exit
1733254891547:show javaVersion
1733254905295:show javaHome
1733254925248:java -version
1733254928434:exit
1733255684969:clean
1733255687494:compile
1733255787671:run -- -rt json -rc ./config.json
1733255846070:run -rt json -rc ./config.json
1733255957829:run -rt json -rc Configuration/reader_json.json
1733256070666:run
1733256159700:exit
